# WebhookHub ğŸš€

**Production-grade Webhook Ingestion, Delivery & Retry Platform**

WebhookHub is a **multi-tenant, reliable webhook infrastructure** inspired by Stripe, GitHub, and Slack webhook systems.
It provides **safe ingestion**, **fan-out delivery**, **per-subscription retries**, and **dead-letter handling** for event-driven integrations.

---

## ğŸ“Œ Why WebhookHub?

Building webhooks correctly is hard. Common issues include:

* Duplicate webhook events
* No idempotency guarantees
* Retry storms
* Partial delivery failures
* Lack of observability
* No DLQ or replay mechanism

WebhookHub solves these problems by offering a **robust, scalable, and fault-tolerant webhook system** suitable for SaaS products.

---

## âœ¨ Key Features

* Multi-tenant webhook ingestion
* Provider signature verification (Stripe-style)
* Idempotent event ingestion
* Kafka-based event fan-out
* Per-subscription delivery tracking
* Exponential retry with backoff
* Dead Letter Queue (DLQ)
* Horizontal worker scalability
* Fully Dockerized local setup

---

### ğŸŒ³ Repository Branching Strategy

WebhookHub follows a trunk-based, environment-aligned branching strategy designed for fast iteration, safe deployments, and clear ownership as the system scales.

ğŸ“Œ Branch Overview
* main        â†’ Production-ready code
* develop     â†’ Active development, integration & testing
* feature/*   â†’ New features or improvements
* fix/*       â†’ Bug fixes
* hotfix/*    â†’ Critical production fixes

---

## ğŸ§± System Architecture

### High-Level Architecture

```mermaid
flowchart LR
    Provider -->|Webhook| API
    API -->|Store| Postgres
    API -->|Publish| Kafka
    Kafka --> Worker
    Worker -->|HTTP| Client
    Worker --> RetryWorker
    RetryWorker -->|On failure| DLQ
```

### Webhook Ingestion Flow (Stripe â†’ WebhookHub API)

```mermaid
sequenceDiagram
    autonumber
    participant Stripe
    participant API as WebhookHub API
    participant DB as PostgreSQL
    participant Kafka

    Stripe->>API: POST /webhooks/stripe/{tenantId}
    API->>API: Capture raw body
    API->>API: Verify Stripe signature (HMAC)

    alt Invalid signature
        API-->>Stripe: 400 Bad Request
    else Valid signature
        API->>DB: BEGIN TRANSACTION
        API->>DB: Check idempotency (events_state)
        alt Duplicate event
            API->>DB: ROLLBACK
            API-->>Stripe: 200 OK (already processed)
        else New event
            API->>DB: Insert events_raw
            API->>DB: Insert events_state (RECEIVED)
            API->>DB: COMMIT
            API->>Kafka: Publish event (events_ingested)
            API-->>Stripe: 202 Accepted
        end
    end
```

### Kafka â†’ Worker Processing Flow
```mermaid
sequenceDiagram
    autonumber
    participant Kafka
    participant Worker
    participant DB as PostgreSQL

    Kafka->>Worker: Consume event (events_ingested)
    Worker->>DB: Fetch event state

    alt Already processed
        Worker-->>Kafka: Commit offset
    else New event
        Worker->>DB: Update state â†’ PROCESSING
        Worker->>Worker: Process event (provider-specific)

        alt Processing success
            Worker->>DB: Update state â†’ SUCCESS
            Worker-->>Kafka: Commit offset
        else Processing failed
            Worker->>DB: Increment retry_count
            Worker->>DB: Update state â†’ FAILED
            Worker-->>Kafka: Commit offset
        end
    end
```

### Retry & DLQ Flow
```mermaid
sequenceDiagram
    autonumber
    participant Worker
    participant Redis
    participant DB as PostgreSQL
    participant Kafka

    Worker->>DB: Detect FAILED event
    Worker->>Redis: Calculate backoff delay

    alt retry_count < MAX_RETRIES
        Redis-->>Worker: Delay expires
        Worker->>Kafka: Re-publish event
        Worker->>DB: Update state â†’ RECEIVED
    else retry_count >= MAX_RETRIES
        Worker->>DB: Insert into events_dlq
        Worker->>DB: Update state â†’ DLQ
    end
```

### End-to-End WebhookHub System Diagram
```mermaid
sequenceDiagram
    autonumber
    participant Provider
    participant API
    participant DB
    participant Kafka
    participant Worker
    participant Client

    Provider->>API: Webhook Event
    API->>DB: Store raw event + state
    API->>Kafka: Publish event
    API-->>Provider: 202 Accepted

    Kafka->>Worker: Consume event
    Worker->>DB: PROCESSING
    Worker->>Client: Deliver webhook
    alt Client success
        Worker->>DB: SUCCESS
    else Client failure
        Worker->>DB: FAILED / DLQ
    end
```

### Event State Machine
```mermaid
stateDiagram-v2
    [*] --> RECEIVED
    RECEIVED --> PROCESSING
    PROCESSING --> SUCCESS
    PROCESSING --> FAILED
    FAILED --> RECEIVED : retry
    FAILED --> DLQ : max retries
    DLQ --> [*]
    SUCCESS --> [*]
```

---

## ğŸ§° Tech Stack

| Layer            | Technology                   |
| ---------------- | ---------------------------- |
| Backend          | Node.js, TypeScript, Fastify |
| Messaging        | Apache Kafka                 |
| Database         | PostgreSQL                   |
| Retry Scheduling | DB-based backoff             |
| Containerization | Docker, Docker Compose       |
| Logging          | Structured JSON logging      |

---

## ğŸ“‚ Repository Structure

```txt
webhookhub/
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ .env.example
â”œâ”€â”€ README.md
â”œâ”€â”€ Makefile
â”‚
â”œâ”€â”€ api/                        # Ingestion Service (Fastify)
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ tsconfig.json
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ server.ts           # App bootstrap
â”‚       â”œâ”€â”€ app.ts              # Fastify instance & plugins
â”‚       â”‚
â”‚       â”œâ”€â”€ routes/
â”‚       â”‚   â”œâ”€â”€ index.ts
â”‚       â”‚   â””â”€â”€ webhook.routes.ts
â”‚       â”‚
â”‚       â”œâ”€â”€ modules/
â”‚       â”‚   â””â”€â”€ webhook/
â”‚       â”‚       â”œâ”€â”€ webhook.controller.ts
â”‚       â”‚       â”œâ”€â”€ webhook.service.ts
â”‚       â”‚       â”œâ”€â”€ webhook.schema.ts
â”‚       â”‚       â”œâ”€â”€ webhook.repository.ts
â”‚       â”‚       â””â”€â”€ webhook.types.ts
â”‚       â”‚
â”‚       â”œâ”€â”€ infra/              # Cross-cutting infrastructure
â”‚       â”‚   â”œâ”€â”€ db.ts
â”‚       â”‚   â”œâ”€â”€ kafka.ts
â”‚       â”‚   â”œâ”€â”€ redis.ts
â”‚       â”‚   â”œâ”€â”€ logger.ts
â”‚       â”‚   â”œâ”€â”€ tracer.ts
â”‚       â”‚   â””â”€â”€ metrics.ts
â”‚       â”‚
â”‚       â”œâ”€â”€ middlewares/
â”‚       â”‚   â”œâ”€â”€ rateLimit.ts
â”‚       â”‚   â”œâ”€â”€ signatureVerify.ts
â”‚       â”‚   â””â”€â”€ errorHandler.ts
â”‚       â”‚
â”‚       â”œâ”€â”€ config/
â”‚       â”‚   â”œâ”€â”€ env.ts
â”‚       â”‚   â””â”€â”€ constants.ts
â”‚       â”‚
â”‚       â””â”€â”€ utils/
â”‚           â”œâ”€â”€ hmac.ts
â”‚           â”œâ”€â”€ idGenerator.ts
â”‚           â””â”€â”€ time.ts
â”‚
â”œâ”€â”€ worker/                     # Async Processing Service
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ tsconfig.json
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ index.ts            # Worker bootstrap
â”‚       â”‚
â”‚       â”œâ”€â”€ consumers/
â”‚       â”‚   â”œâ”€â”€ event.consumer.ts
â”‚       â”‚   â””â”€â”€ retry.consumer.ts
â”‚       â”‚
â”‚       â”œâ”€â”€ processors/
â”‚       â”‚   â””â”€â”€ stripe.processor.ts
â”‚       â”‚
â”‚       â”œâ”€â”€ state/
â”‚       â”‚   â””â”€â”€ event.state.ts  # State machine logic
â”‚       â”‚
â”‚       â”œâ”€â”€ infra/
â”‚       â”‚   â”œâ”€â”€ db.ts
â”‚       â”‚   â”œâ”€â”€ kafka.ts
â”‚       â”‚   â”œâ”€â”€ redis.ts
â”‚       â”‚   â”œâ”€â”€ logger.ts
â”‚       â”‚   â””â”€â”€ tracer.ts
â”‚       â”‚
â”‚       â”œâ”€â”€ services/
â”‚       â”‚   â”œâ”€â”€ delivery.service.ts
â”‚       â”‚   â”œâ”€â”€ retry.service.ts
â”‚       â”‚   â””â”€â”€ dlq.service.ts
â”‚       â”‚
â”‚       â”œâ”€â”€ config/
â”‚       â”‚   â””â”€â”€ env.ts
â”‚       â”‚
â”‚       â””â”€â”€ utils/
â”‚           â””â”€â”€ backoff.ts
â”‚
â”œâ”€â”€ infra/                      # Platform Infrastructure
â”‚   â”œâ”€â”€ prometheus.yml
â”‚   â”œâ”€â”€ grafana/
â”‚   â”‚   â””â”€â”€ dashboards/
â”‚   â”‚       â””â”€â”€ webhookhub.json
â”‚   â””â”€â”€ nginx/
â”‚       â””â”€â”€ nginx.conf
â”‚
â”œâ”€â”€ db/
â”‚   â”œâ”€â”€ migrations/
â”‚   â”‚   â”œâ”€â”€ 001_events_raw.sql
â”‚   â”‚   â”œâ”€â”€ 002_events_state.sql
â”‚   â”‚   â””â”€â”€ 003_events_dlq.sql
â”‚   â””â”€â”€ seeds/
â”‚       â””â”€â”€ tenants.sql
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ create-topics.sh
â”‚   â”œâ”€â”€ init-db.sh
â”‚   â””â”€â”€ replay-event.sh
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/
â”‚   â”‚   â”œâ”€â”€ signature.test.ts
â”‚   â”‚   â””â”€â”€ idempotency.test.ts
â”‚   â””â”€â”€ integration/
â”‚       â””â”€â”€ webhook-flow.test.ts
â”‚
â””â”€â”€ docs/
    â”œâ”€â”€ architecture.md
    â”œâ”€â”€ api-contracts.md
    â””â”€â”€ failure-scenarios.md

```

---

## ğŸ—„ï¸ Data Model Overview

### Core Tables

| Table                   | Purpose                            |
| ----------------------- | ---------------------------------- |
| `tenants`               | SaaS tenants                       |
| `events_raw`            | Immutable webhook ingestion log    |
| `events_state`          | Event lifecycle state              |
| `webhook_subscriptions` | Client webhook endpoints           |
| `event_deliveries`      | Per-subscription delivery tracking |
| `events_dlq`            | Failed events (Dead Letter Queue)  |

For more info explore db-migrate folder of the repo.
---

## ğŸ” End-to-End Flow

### 1ï¸âƒ£ Webhook Ingestion

1. Provider sends webhook
2. Signature is verified
3. Event deduplicated
4. Stored in `events_raw`
5. State initialized in `events_state`
6. Event published to Kafka

---

### 2ï¸âƒ£ Event Fan-out (Kafka)

* Kafka consumer receives event
* Active subscriptions are fetched
* Delivery records created in `event_deliveries`

---

### 3ï¸âƒ£ Delivery Execution

* Each subscription is delivered **independently**
* Payload is signed before delivery
* Status updated to `SUCCESS` or `FAILED`

---

### 4ï¸âƒ£ Retry Handling

* Failed deliveries are scheduled using **exponential backoff**
* Retry metadata stored in DB (`retry_count`, `next_retry_at`)
* Retry worker picks eligible deliveries

---

### 5ï¸âƒ£ Dead Letter Queue (DLQ)

* Deliveries exceeding max retries are moved to `events_dlq`
* DLQ enables debugging, alerts, and future replay

---

## ğŸ” Retry Strategy

WebhookHub uses **exponential backoff**:

```txt
delay = baseDelay * (2 ^ retryCount)
```

Retries are:

* Per-delivery (not per event)
* Bounded by a configurable max retry count
* Safe from retry storms

---

## ğŸ›¡ï¸ Idempotency Guarantees

* **Ingestion** is idempotent using `(tenant_id, provider, event_id)`
* **Delivery** is idempotent per `(event_id, subscription_id)`
* Kafka offsets are committed only after safe processing

---

## âš™ï¸ Local Development Setup

### Prerequisites

* Docker
* Docker Compose

---

### Start All Services

```bash
docker-compose up --build
```

This starts:

* PostgreSQL
* Kafka + Zookeeper
* API service
* Worker service
* Retry worker

---

### Verify Services

| Service  | URL                                            |
| -------- | ---------------------------------------------- |
| API      | [http://localhost:8080](http://localhost:8080) |
| Postgres | localhost:5432                                 |
| Kafka    | localhost:9092                                 |

---

## ğŸ§ª Testing the System

### 1ï¸âƒ£ Create a Webhook Subscription

```sql
INSERT INTO webhook_subscriptions (
  id, tenant_id, provider, event_type, target_url, secret
)
VALUES (
  gen_random_uuid(),
  'stashfin',
  'stripe',
  'payment.failed',
  'http://localhost:9999/fail',
  'test_secret'
);
```

---

### 2ï¸âƒ£ Send Test Webhook

```bash
curl -X POST http://localhost:8080/webhooks/stripe/stashfin \
  -H "Content-Type: application/json" \
  -H "Stripe-Signature: test" \
  -d '{
    "id": "evt_test_001",
    "type": "payment.failed",
    "data": { "reason": "card_declined" }
  }'
```

---

### 3ï¸âƒ£ Observe Behavior

* Event stored in `events_raw`
* Delivery attempts recorded
* Retries scheduled
* DLQ populated after max retries

---

## ğŸ’¥ Failure Scenarios Covered

* Duplicate webhook events
* Subscriber downtime
* Partial delivery failures
* Retry exhaustion
* Kafka consumer restarts
* Worker crashes

---

## ğŸ“ˆ Scaling & Production Considerations

* Horizontal scaling of workers
* Kafka partitioning
* DB indexes for retry scans
* Observability hooks
* DLQ alerts & replay tooling

---

## ğŸš€ Future Enhancements

* SaaS dashboard UI
* Client API keys & OAuth
* Webhook replay from DLQ
* Metrics (Prometheus)
* Rate limiting per tenant

---

## ğŸ¯ Engineering Takeaways

WebhookHub demonstrates:

* Event-driven system design
* Safe distributed retries
* Idempotent workflows
* Kafka consumer correctness
* Production-grade failure handling

---

## ğŸ‘¤ Author

Built by **Rituraj Debnath**
Backend Engineer | Distributed Systems | Event-Driven Architecture
